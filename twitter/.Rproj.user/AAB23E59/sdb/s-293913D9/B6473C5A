{
    "collab_server" : "",
    "contents" : "#' function popular_tweets and subfunctions and codes, designed to make other explorations\n#' of four movies' tweet data possible\n#' Step1: Search tweets about four movies\n#' Step2: Clean the data about these tweets\n#' Step3: Show the realtionship between favourite count and retweet count for each movie\n#' Step4: Show 5 most popular tweet sources for each movie(use functions here)\n#' Step5: Get the most 5 popular sources of tweets about each movie\n#' Step6: plot time series of tweets about'Gangs of New York'\n#' Step7: Graphical Visualizations\n#' Step8: Get Timelines for each user since 2017-12-01\n#' Step9: Text Cleaning, removing URLS, stopwords, extra whitespace, etc\n#' Step10: Plot the frequent Words of all the tweets\n#' Step11: Wordcloud\n#' @param x movie_name.\n#' @return the most 5 popular tweet sources of \\code{x}.\n#' @export\n#' @examples\n#' popular_sources(\"Gangs of New York\")\n#' popular_sources(\"Sleepless in Seattle\")\n\n\nlibrary(rtweet)\n\nNY <- search_tweets(\n  '\"Gangs of New York\"', n = 250000, retryonratelimit = TRUE,since='2017-12-01', until='2017-12-11')\n\nSeattle <- search_tweets(\n  '\"Sleepless in Seattle\"', n = 500000, retryonratelimit = TRUE,since='2017-12-01', until='2017-12-11'\n)\n\nLA <- search_tweets(\n  '\"L.A. Confidential\"', n = 250000, retryonratelimit = TRUE,since='2017-12-01', until='2017-12-11'\n)\n\nLasVegas <- search_tweets(\n  '\"Fear and Loathing in Las Vegas\"', n = 250000, retryonratelimit = TRUE,since='2017-12-01', until='2017-12-11'\n)\n\n\n\n##Data Cleaning\n\nlibrary(dplyr)\nlibrary(ggplot2)\nnames(NY)\n##comine 4 datasets\nNY[\"movie\"] <- \"Gangs of New York\"\nLA[\"movie\"] <- \"L.A. Confidential\"\nSeattle[\"movie\"] <- \"Sleepless in Seattle\"\nLasVegas[\"movie\"] <- \"Fear and Loathing in Las Vegas\"\nrawdata <- rbind(NY,LA,Seattle,LasVegas)\n\n##select columns and filter columns\n\nmovies <- rawdata[, c(\"movie\",\"user_id\",\"text\",\"source\",\"favorite_count\",\"retweet_count\",\"lang\",\"country\",\"country_code\",\"geo_coords\",\"coords_coords\",\"bbox_coords\",\"created_at\")] %>% filter(lang =='en') %>% filter(!is.na(text)) %>% as.data.frame()\n\n\n##Data Visulization\n\n## Show the realtionship between favourite count and retweet count for each movie\nmovies_by_name <- movies%>% group_by(movie)%>%filter(favorite_count !=0 & retweet_count !=0 &retweet_count<50)%>%arrange(favorite_count)\n\nlibrary(ggplot2)\n\nggplot(movies_by_name,aes(favorite_count,retweet_count,group = movie, colour = movie)) + geom_point()+geom_smooth(method = \"loess\")\n\n##show 5 most popular tweet sources for each movie(use functions here)\npopular_sources<- function(movie_name=\"Gangs of New York\"){\n  movies_by_source <- subset(movies,movie == movie_name)%>%\n    group_by(source)%>%\n    summarise(favorite_count=sum(favorite_count))%>%\n    arrange(desc(favorite_count))%>%\n    head(5)%>%as.data.frame()\n  movies_by_source$movie <- movie_name\n  return(movies_by_source)\n}\n\n\n\npopular_sources()\npopular_sources(movie_name=\"Sleepless in Seattle\")\npopular_sources(movie_name=\"L.A. Confidential\")\npopular_sources(\"Fear and Loathing in Las Vegas\")\n\n## plot time series of tweets about'Gangs of New York'\nts_plot(NY, \"3 hours\") +\n  ggplot2::theme_minimal() +\n  ggplot2::theme(plot.title = ggplot2::element_text(face = \"bold\")) +\n  ggplot2::labs(\n    x = NULL, y = NULL,\n    title = \"Frequency of Twitter about 'Gangs of New York' statuses from past 10 days\",\n    subtitle = \"Twitter status (tweet) counts aggregated using three-hour intervals\",\n    caption = \"\\nSource: Data collected from Twitter's REST API via rtweet\"\n  )\n\n\n##Graphical Visualizations\n\n## create lat/lng variables using all available tweet and profile geo-location data\nrt <- lat_lng(movies)\n\n## plot state boundaries\npar(mar = c(0, 0, 0, 0))\nmaps::map(\"state\", lwd = .25)\n\n## plot lat and lng points onto state map\nwith(rt, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))\n\n\n##Get Timelines\n\n## Get the most recent 3,200 tweets from cnn, BBCWorld, and foxnews\ncities <- get_timelines(c(\"nytimes\", \"seattletimes \",\"CityOfLasVegas\",\"latimes\"), n = 3200)\n\n## plot the frequency of tweets for each user since 2017-12-01\ncities %>%\n  dplyr::filter(created_at > \"2017-12-01\") %>%\n  dplyr::group_by(screen_name) %>%\n  ts_plot(\"days\", trim = 1L) +\n  ggplot2::geom_point() +\n  ggplot2::theme_minimal() +\n  ggplot2::theme(\n    legend.title = ggplot2::element_blank(),\n    legend.position = \"bottom\",\n    plot.title = ggplot2::element_text(face = \"bold\")) +\n  ggplot2::labs(\n    x = NULL, y = NULL,\n    title = \"Frequency of Twitter statuses posted by news organization\",\n    subtitle = \"Twitter status (tweet) counts aggregated by day from October/November 2017\",\n    caption = \"\\nSource: Data collected from Twitter's REST API via rtweet\"\n  )\n\n\n##Text Cleaning\n\n\nlibrary(tm)\n# build a corpus, and specify the source to be character vectors\nmyCorpus <- Corpus(VectorSource(movies$text))\n# convert to lower case\nmyCorpus <- tm_map(myCorpus, content_transformer(tolower))\n# remove URLs\nremoveURL <- function(x) gsub(\"http[^[:space:]]*\", \"\", x)\nmyCorpus <- tm_map(myCorpus, content_transformer(removeURL))\n# remove anything other than English letters or space\nremoveNumPunct <- function(x) gsub(\"[^[:alpha:][:space:]]*\", \"\", x)\nmyCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))\n# remove stopwords\nmyStopwords <- c(setdiff(stopwords('english'), c(\"r\", \"big\")),\n                 \"use\", \"see\", \"used\", \"via\", \"amp\")\nmyCorpus <- tm_map(myCorpus, removeWords, myStopwords)\n# remove extra whitespace\nmyCorpus <- tm_map(myCorpus, stripWhitespace)\nmyCorpusCopy <- myCorpus\ntdm<- TermDocumentMatrix(myCorpus,\n                         control = list(wordLengths = c(1, Inf)))\ntdm\n\n\n##Frequent Words\n\nfreq.terms <- findFreqTerms(tdm, lowfreq = 20)\nfreq.terms\nterm.freq <- rowSums(as.matrix(tdm))\nterm.freq <- subset(term.freq, term.freq >= 20)\ndf <- data.frame(term = names(term.freq), freq = term.freq)%>%head(15)\nlibrary(ggplot2)\nggplot(df, aes(x=term, y=freq)) + geom_bar(stat=\"identity\") +\n  xlab(\"Terms\") + ylab(\"Count\") + coord_flip() +\n  theme(axis.text=element_text(size=7))\n\nm <- as.matrix(tdm)\nm <- m %>%head(400)\n# calculate the frequency of words and sort it by frequency w\nword.freq <- sort(rowSums(m), decreasing = T)\n\n\n##Wordcloud\n\nlibrary(wordcloud)\nwordcloud(words = names(word.freq), freq = word.freq, min.freq = 3,random.order = F)\n\n\n\n",
    "created" : 1513143534527.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "172147917",
    "id" : "B6473C5A",
    "lastKnownWriteTime" : 1513143705,
    "last_content_update" : 1513143705183,
    "path" : "~/twitter/R/code.R",
    "project_path" : "R/code.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}